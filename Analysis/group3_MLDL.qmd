### 目標
預測五個變數(Bone, Brain, Kidney, Liver, Lung)是否有轉移。

### 方法
- 深度學習: Multi-head Neural Network
- 機器學習: Decision Tree, Random Forest, LightGBM, CatBoost   

### 預處理 
#### 劃分資料集
將資料按照9:1切分訓練集與測試集。針對五個目標變數的轉移狀態，此資料共有28種可能的狀態組合。採用分層切分的方式，確保各個組合在訓練集與測試集的比例盡量保持一致。  

```{python}
#| echo: false
# preprocessing
import pandas as pd
from Group3_MLDL.preprocess import preprocess_data, stratify_plot, stratify_stack_plot
import warnings
warnings.filterwarnings("ignore")
```

```{python}
#| echo: false
# 載入資料
genes_path = 'Group3_MLDL/Metastasis/genes.csv'
meta_path = 'Group3_MLDL/Metastasis/metabolites.csv'
resp_path = 'Group3_MLDL/Metastasis/Response.csv'

# 處理資料
X_train, X_test, y_train, y_test, colnames = preprocess_data(genes_path, meta_path, resp_path, stratify=True)
# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
cancers = ['Bone', 'Brain', 'Kidney', 'Liver', 'Lung']

# 切分資料集後 有轉移&無轉移的比例
stratify_stack_plot(y_train, y_test) 
```

由上圖發現，`Brain`存在些許資料不平衡問題(1.89倍)，其餘變數則不是很明顯，後續分析將嘗試針對`Brain`變數做上採樣(SMOTE)。


### Multi-head Neural Network
#### 目標 
希望做到一個模型同時預測多個目標變數。  

#### 模型設計
##### 共享層
包含一個全連接層，配合ReLU激活函數，其主要作用是提取輸入資料中的通用特徵。這層的輸出維度設定為128維。

##### 五個獨立的輸出層
在共享層之後，架構分出五個獨立的輸出層，每個輸出層都由一個全連接層構成。每個輸出層都專門負責預測一個特定的目標變數。這種設計允許模型對每個目標進行專門的學習和預測，同時基於共享層的特徵，加強模型對各目標之間可能存在的隱含關聯的理解。

##### 損失函數
採用二元交叉熵（Binary Cross-Entropy）作為損失函數，對每個獨立輸出層的預測結果計算損失。由於這是一個多目標的預測任務，模型會計算所有輸出層的損失總和，以此來進行梯度下降並更新網路的權重。

##### 優化器
Adam

##### 學習率
0.0005  

![Multi-head Neural Network](Group3_MLDL/plot/NN.png){width="50%" align="center"}  
共有 $(4659 + 1) \times 128  + (128 + 1) \times 5 = 597125$ 個參數待估計。

#### 模型表現
```{python}
#| echo: false
# DL
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from Group3_MLDL.dl import MultiHeadNN, train, test, evaluate, evaluate_plot
import numpy as np
import random
```

```{python}
#| echo: false
# 設定隨機種子
seed = 123
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

# 超參數 
shared = 128 # 共享層維度
num_epochs = 200
batch_size = 4
trytry = 2 # 第幾次測試

# 轉換為 PyTorch 張量
X_train_tensor = torch.tensor(X_train).float()
X_test_tensor = torch.tensor(X_test).float()
y_train_tensor = torch.tensor(y_train).float()
y_test_tensor = torch.tensor(y_test).float()

# 初始化模型、損失函數和優化器
model = MultiHeadNN(input_dim=X_train_tensor.shape[1], shared_dim=shared, output_dim=1)
criterion = nn.BCEWithLogitsLoss() # sigmoid + BCELoss
optimizer = optim.Adam(model.parameters(), lr=0.001)
```
```{python}
#| echo: false
# 開始訓練
performance_history = []
loss_history = []
for epoch in range(num_epochs):
    # 訓練模型
    loss = train(model, X_train_tensor, y_train_tensor, criterion, optimizer, batch_size)
    # 測試模型
    outputs = test(model, X_test_tensor)
    # 評估模型
    epoch_metrics = evaluate(outputs, y_test_tensor.cpu().numpy(), cancers)
    
    loss_history.append(loss)
    performance_history.append(epoch_metrics)
    formatted_auc = ', '.join([f"{i}: {epoch_metrics[i]['AUC']:.4f}" for i in epoch_metrics.keys()])
    # print(f'【Epoch】{epoch+1}/{num_epochs}, 【Train Loss】{loss:.4f}, 【Test AUC】{formatted_auc}')


# 儲存模型
torch.save(model, f'Group3_MLDL/model/{trytry}_model.pt')
# print(f"已儲存模型至model/{trytry}_model.pt")

# 匯入模型
# model = torch.load('model/1_model.pt')
```

```{python}
#| echo: false
# 繪製Loss, AUC
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), loss_history)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title(f"Training Loss")
plt.subplot(1, 2, 2)
evaluate_plot(performance_history, "AUC", cancers)

plt.savefig(f'Group3_MLDL/plot/lossAUC_{trytry}.png', dpi=300)
# print(f"已儲存圖片至plot/lossAUC_{trytry}.png")
```
```{python}
#| echo: false
# 繪製ACC, Precision, Recall, F1
plt.figure(figsize=(18, 4))
plt.subplot(1, 4, 1)
evaluate_plot(performance_history, "ACC", cancers)
plt.subplot(1, 4, 2)
evaluate_plot(performance_history, "Precision", cancers)
plt.subplot(1, 4, 3)
evaluate_plot(performance_history, "Recall", cancers)
plt.subplot(1, 4, 4)
evaluate_plot(performance_history, "F1", cancers)

plt.savefig(f'Group3_MLDL/plot/other_{trytry}.png', dpi=300)
# print(f"已儲存圖片至plot/other_{trytry}.png")
```


深度學習模型對隨機初始值特別敏感，尤其當樣本數較少時，這一點更為明顯。在此次實驗的五個目標變數的預測中，AUC值的波動範圍從0.4到0.7不等。但若是初始值設定不佳，模型有時會在初期傾向將所有樣本預測為全正或全負，導致預測結果極不穩定。因此，在樣本量有限的情況下，依賴深度學習可能不是理想的選擇。  


### Machine Learning Method
#### 目標
為目標變數: Bone, Brain, Kidney, Liver, Lung 分別建立五個模型，並觀察對這五個目標最有影響的變數。  

#### 方法
由於希望考慮交互作用項，以及後續方便解釋特徵重要性，這裡皆使用Tree-based 模型。

- Decision Tree
- Random Forest 
- LightGBM
- CatBoost  

```{python}
#| echo: false
# ML
from Group3_MLDL.ml import check_transfer, choose_model, featureplot, afterSMOTE_models
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import pickle
```

#### Decision Tree
```{python}
#| echo: false
# 決策樹
dt_results = check_transfer(X_train, X_test, y_train, y_test, "DecisionTree")
dt_results
```

#### Random Forest
```{python}
#| echo: false
# 隨機森林
rf_results = check_transfer(X_train, X_test, y_train, y_test, "RandomForest")
rf_results
```

#### LightGBM
```{python}
#| echo: false
# LightGBM
# lgbm_results = check_transfer(X_train, X_test, y_train, y_test, "LightGBM") # 於server上運算
lgbm_results = pd.read_csv('Group3_MLDL/result/LightGBM_transfer.csv', index_col=0)
lgbm_results
```

#### CatBoost
```{python}
#| echo: false
# CatBoost (計算時間較長 約7分鐘)
# catb_results = check_transfer(X_train, X_test, y_train, y_test, "CatBoost") # 於server上運算
catb_results = pd.read_csv('Group3_MLDL/result/CatBoost_transfer.csv', index_col=0)
catb_results
```

#### SMOTE
僅針對`Brain`變數做上採樣，將少類的樣本補到多數類的80%。  

```{python}
#| echo: false
# SMOTE (只做Brain部位)
smote = SMOTE(random_state=0, sampling_strategy=0.8)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train[:,1])

print('Brain')
print(f'原資料: 轉移 {np.bincount(y_train[:,1])[0]}, 沒轉移 {np.bincount(y_train[:,1])[1]} ')
print(f"SMOTE後: 轉移 {np.bincount(y_train_smote)[0]}, 沒轉移 {np.bincount(y_train_smote)[1]} ")

models = ['DecisionTree', 'RandomForest', 'LightGBM', 'CatBoost']
# smo_df = afterSMOTE_models(models, 'Brain', X_train_smote, X_test, y_train_smote, y_test) # 於server上運算

smo_df = pd.read_csv('Group3_MLDL/result/Brain_smote.csv', index_col=0)
smo_df
```

將做過SMOTE上採樣的資料進行訓練，所有模型的表現皆提高了。除了DecisionTree只有微幅上升以外，其餘模型RandomForest, LightGBM, CatBoost皆提高了0.2以上。

#### 各個目標變數對應AUC最高之模型
```{python}
#| echo: false
# 選擇AUC最高的模型
df = choose_model(dt_results, rf_results, lgbm_results, catb_results, smo_df)
df
```

以下為針對不同目標變數預測轉移的最佳模型選擇： 

- 對於是否轉移到「骨頭」，**隨機森林**模型表現最佳，AUC可達66.56%。
- 對於是否轉移到「大腦」，結合**SMOTE**與**隨機森林**模型能夠達到最佳效果，AUC可達76.95%。
- 對於是否轉移到「腎臟」，**隨機森林**模型表現最佳，AUC可達71.15%。
- 對於是否轉移到「肝臟」，「**隨機森林**模型表現最佳，AUC可達65.38%。
- 對於是否轉移到「肺臟」，**CatBoost**模型表現最佳，AUC為56.41%。



### 對是否轉移影響最大的前20個變數
```{python}
#| echo: false
with open('Group3_MLDL/model/Bone_RandomForest.pkl', 'rb') as file:
    RandomForest_Bone = pickle.load(file)
with open('Group3_MLDL/model/Brain_RandomForest.pkl', 'rb') as file:
    RandomForest_Brain = pickle.load(file)
with open('Group3_MLDL/model/Kidney_RandomForest.pkl', 'rb') as file:
    RandomForest_Kidney = pickle.load(file)
with open('Group3_MLDL/model/Liver_RandomForest.pkl', 'rb') as file:
    RandomForest_Liver = pickle.load(file)
with open('Group3_MLDL/model/Lung_CatBoost.pkl', 'rb') as file:
    CatBoost_Lung = pickle.load(file)
```

```{python echo=FALSE}
# 重要變數
featureplot(RandomForest_Bone.feature_importances_,20, colnames, "Bone (RandomForest) Feature Importance")
featureplot(RandomForest_Brain.feature_importances_,20, colnames, "Brain (Smote+RandomForest) Feature Importance")
featureplot(RandomForest_Kidney.feature_importances_,20, colnames, "Kidney (RandomForest) Feature Importance")
featureplot(RandomForest_Liver.feature_importances_,20, colnames, "Liver (RandomForest) Feature Importance")
featureplot(CatBoost_Lung.feature_importances_,20, colnames, "Lung (CatBoost) Feature Importance")
```

- 預測是否轉移至「骨頭」，重要的代謝體為: **C34.4.PC**
- 預測是否轉移至「大腦」，重要的代謝體為: **C54.6.TAG**, **C58.8.TAG**, **asparagine**, **C56.6.TAG**, **C54.4.TAG**
- 預測是否轉移至「腎臟」，重要的代謝體為: **homocysteine**, **cytidine**
- 預測是否轉移至「肝臟」，重要的代謝體為: **GABA**, **putrescine**, **cytidine**
- 預測是否轉移至「肺臟」，重要的代謝體為: **taurodeoxycholate.taurochenodeoxycholate**, **C36.1.PC**, **C54.6.TAG**, **succinate.methylmalonate**