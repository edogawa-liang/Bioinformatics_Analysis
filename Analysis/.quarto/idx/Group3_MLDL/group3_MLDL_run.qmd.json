{"title":"Analysis using ML, DL","markdown":{"yaml":{"title":"Analysis using ML, DL","subtitle":"Group 3","date":"today","author":"尹子維、李承祐、黃亮臻、張立勳","format":{"pdf":{"include-in-header":[{"text":"\\usepackage{setspace,relsize}\n\\usepackage{xeCJK}\n\\setCJKmainfont{STSong}\n"}]}},"mainfont":"STSong","toc":false,"pdf_document":{"latex_engine":"xelatex","geometry":["left=30mm","right=30mm","top=30mm","bottom=30mm"]}},"headingText":"目標","containsRefs":false,"markdown":"\n預測五個變數(Bone, Brain, Kidney, Liver, Lung)是否有轉移。\n\n### 方法\n- 深度學習: Multi-head Neural Network\n- 機器學習: Decision Tree, Random Forest, LightGBM, CatBoost   \n\n### 預處理 \n#### 劃分資料集\n將資料按照9:1切分訓練集與測試集。針對五個目標變數的轉移狀態，此資料共有28種可能的狀態組合。採用分層切分的方式，確保各個組合在訓練集與測試集的比例盡量保持一致。  \n```{python}\n#| echo: false\n# preprocessing\nimport pandas as pd\nfrom preprocess import preprocess_data, stratify_plot, stratify_stack_plot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n```{python}\n#| echo: false\n# 載入資料\ngenes_path = 'Metastasis/genes.csv'\nmeta_path = 'Metastasis/metabolites.csv'\nresp_path = 'Metastasis/Response.csv'\n\n# 處理資料\nX_train, X_test, y_train, y_test, colnames = preprocess_data(genes_path, meta_path, resp_path, stratify=True)\n# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\ncancers = ['Bone', 'Brain', 'Kidney', 'Liver', 'Lung']\n\n# 切分資料集後 有轉移&無轉移的比例\nstratify_stack_plot(y_train, y_test) \n```\n\n由上圖發現，`Brain`存在些許資料不平衡問題(1.89倍)，其餘變數則不是很明顯，後續分析將嘗試針對`Brain`變數做上採樣(SMOTE)。\n\n\n### Multi-head Neural Network\n#### 目標 \n希望做到一個模型同時預測多個目標變數。  \n\n#### 模型設計\n##### 共享層\n包含一個全連接層，配合ReLU激活函數，其主要作用是提取輸入資料中的通用特徵。這層的輸出維度設定為128維。\n\n##### 五個獨立的輸出層\n在共享層之後，架構分出五個獨立的輸出層，每個輸出層都由一個全連接層構成。每個輸出層都專門負責預測一個特定的目標變數。這種設計允許模型對每個目標進行專門的學習和預測，同時基於共享層的特徵，加強模型對各目標之間可能存在的隱含關聯的理解。\n\n##### 損失函數\n採用二元交叉熵（Binary Cross-Entropy）作為損失函數，對每個獨立輸出層的預測結果計算損失。由於這是一個多目標的預測任務，模型會計算所有輸出層的損失總和，以此來進行梯度下降並更新網路的權重。\n\n##### 優化器\nAdam\n\n##### 學習率\n0.0005  \n\n![Multi-head Neural Network](plot/NN.png){width=\"50%\" align=\"center\"}  \n共有 $4659 \\times 128 + 128 + (128 + 1) \\times 5 = 597125$ 個參數待估計。\n\n#### 模型表現\n```{python}\n#| echo: false\n# DL\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom dl import MultiHeadNN, train, test, evaluate, evaluate_plot\nimport numpy as np\nimport random\n```\n\n```{python}\n#| echo: false\n# 設定隨機種子\nseed = 123\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# 超參數 \nshared = 128 # 共享層維度\nnum_epochs = 200\nbatch_size = 4\ntrytry = 2 # 第幾次測試\n\n# 轉換為 PyTorch 張量\nX_train_tensor = torch.tensor(X_train).float()\nX_test_tensor = torch.tensor(X_test).float()\ny_train_tensor = torch.tensor(y_train).float()\ny_test_tensor = torch.tensor(y_test).float()\n\n# 初始化模型、損失函數和優化器\nmodel = MultiHeadNN(input_dim=X_train_tensor.shape[1], shared_dim=shared, output_dim=1)\ncriterion = nn.BCEWithLogitsLoss() # sigmoid + BCELoss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n```{python}\n#| echo: false\n# 開始訓練\nperformance_history = []\nloss_history = []\nfor epoch in range(num_epochs):\n    # 訓練模型\n    loss = train(model, X_train_tensor, y_train_tensor, criterion, optimizer, batch_size)\n    # 測試模型\n    outputs = test(model, X_test_tensor)\n    # 評估模型\n    epoch_metrics = evaluate(outputs, y_test_tensor.cpu().numpy(), cancers)\n    \n    loss_history.append(loss)\n    performance_history.append(epoch_metrics)\n    formatted_auc = ', '.join([f\"{i}: {epoch_metrics[i]['AUC']:.4f}\" for i in epoch_metrics.keys()])\n    # print(f'【Epoch】{epoch+1}/{num_epochs}, 【Train Loss】{loss:.4f}, 【Test AUC】{formatted_auc}')\n\n\n# 儲存模型\ntorch.save(model, f'model/{trytry}_model.pt')\n# print(f\"已儲存模型至model/{trytry}_model.pt\")\n\n# 匯入模型\n# model = torch.load('model/1_model.pt')\n```\n\n```{python}\n#| echo: false\n# 繪製Loss, AUC\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(num_epochs), loss_history)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(f\"Training Loss\")\nplt.subplot(1, 2, 2)\nevaluate_plot(performance_history, \"AUC\", cancers)\n\nplt.savefig(f'plot/lossAUC_{trytry}.png', dpi=300)\n# print(f\"已儲存圖片至plot/lossAUC_{trytry}.png\")\n```\n```{python}\n#| echo: false\n# 繪製ACC, Precision, Recall, F1\nplt.figure(figsize=(18, 4))\nplt.subplot(1, 4, 1)\nevaluate_plot(performance_history, \"ACC\", cancers)\nplt.subplot(1, 4, 2)\nevaluate_plot(performance_history, \"Precision\", cancers)\nplt.subplot(1, 4, 3)\nevaluate_plot(performance_history, \"Recall\", cancers)\nplt.subplot(1, 4, 4)\nevaluate_plot(performance_history, \"F1\", cancers)\n\nplt.savefig(f'plot/other_{trytry}.png', dpi=300)\n# print(f\"已儲存圖片至plot/other_{trytry}.png\")\n```\n\n\n深度學習模型對隨機初始值特別敏感，尤其當樣本數較少時，這一點更為明顯。在此次實驗的五個目標變數的預測中，AUC值的波動範圍從0.4到0.75不等。但若是初始值設定不佳，模型有時會在初期傾向將所有樣本預測為全正或全負，導致預測結果極不穩定。因此，在樣本量有限的情況下，依賴深度學習可能不是理想的選擇。  \n\n\n### Machine Learning Method\n#### 目標\n為目標變數: Bone, Brain, Kidney, Liver, Lung 分別建立五個模型，並觀察對這五個目標最有影響的變數。  \n\n#### 方法\n由於希望考慮交互作用項，以及後續方便解釋特徵重要性，這裡皆使用Tree-based 模型。\n\n- Decision Tree\n- Random Forest \n- LightGBM\n- CatBoost  \n```{python}\n#| echo: false\n# ML\nfrom ml import check_transfer, choose_model, featureplot, afterSMOTE_models\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nimport pickle\n```\n\n#### Decision Tree\n```{python}\n#| echo: false\n# 決策樹\ndt_results = check_transfer(X_train, X_test, y_train, y_test, \"DecisionTree\")\ndt_results\n```\n\n#### Random Forest\n```{python}\n#| echo: false\n# 隨機森林\nrf_results = check_transfer(X_train, X_test, y_train, y_test, \"RandomForest\")\nrf_results\n```\n\n#### LightGBM\n```{python}\n#| echo: false\n# LightGBM\n# lgbm_results = check_transfer(X_train, X_test, y_train, y_test, \"LightGBM\") # 於server上運算\nlgbm_results = pd.read_csv('result/LightGBM_transfer.csv', index_col=0)\nlgbm_results\n```\n\n#### CatBoost\n```{python}\n#| echo: false\n# CatBoost (計算時間較長 約7分鐘)\n# catb_results = check_transfer(X_train, X_test, y_train, y_test, \"CatBoost\") # 於server上運算\ncatb_results = pd.read_csv('result/CatBoost_transfer.csv', index_col=0)\ncatb_results\n```\n\n#### SMOTE\n僅針對`Brain`變數做上採樣，將少類的樣本補到多數類的80%。\n```{python}\n#| echo: false\n# SMOTE (只做Brain部位)\nsmote = SMOTE(random_state=0, sampling_strategy=0.8)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train[:,1])\n\nprint('Brain')\nprint(f'原資料: 轉移 {np.bincount(y_train[:,1])[0]}, 沒轉移 {np.bincount(y_train[:,1])[1]} ')\nprint(f\"SMOTE後: 轉移 {np.bincount(y_train_smote)[0]}, 沒轉移 {np.bincount(y_train_smote)[1]} \")\n\nmodels = ['DecisionTree', 'RandomForest', 'LightGBM', 'CatBoost']\n# smo_df = afterSMOTE_models(models, 'Brain', X_train_smote, X_test, y_train_smote, y_test) # 於server上運算\n\nsmo_df = pd.read_csv('result/Brain_smote.csv', index_col=0)\nsmo_df\n```\n\n將做過SMOTE上採樣的資料進行訓練，所有模型的表現皆提高了。除了DecisionTree只有微幅上升以外，其餘模型RandomForest, LightGBM, CatBoost皆提高了0.2以上。\n\n#### 各個目標變數對應AUC最高之模型\n```{python}\n#| echo: false\n# 選擇AUC最高的模型\ndf = choose_model(dt_results, rf_results, lgbm_results, catb_results, smo_df)\ndf\n```\n\n以下為針對不同目標變數預測轉移的最佳模型選擇： \n\n- 對於是否轉移到「骨頭」，**隨機森林**模型表現最佳，AUC可達66.56%。\n- 對於是否轉移到「大腦」，結合**SMOTE**與**隨機森林**模型能夠達到最佳效果，AUC可達76.95%。\n- 對於是否轉移到「腎臟」，**隨機森林**模型表現最佳，AUC可達71.15%。\n- 對於是否轉移到「肝臟」，「**隨機森林**模型表現最佳，AUC可達65.38%。\n- 對於是否轉移到「肺臟」，**CatBoost**模型表現最佳，AUC為56.41%。\n\n\n\n### 對是否轉移影響最大的前20個變數\n```{python}\n#| echo: false\nwith open('model/Bone_RandomForest.pkl', 'rb') as file:\n    RandomForest_Bone = pickle.load(file)\nwith open('model/Brain_RandomForest.pkl', 'rb') as file:\n    RandomForest_Brain = pickle.load(file)\nwith open('model/Kidney_RandomForest.pkl', 'rb') as file:\n    RandomForest_Kidney = pickle.load(file)\nwith open('model/Liver_RandomForest.pkl', 'rb') as file:\n    RandomForest_Liver = pickle.load(file)\nwith open('model/Lung_CatBoost.pkl', 'rb') as file:\n    CatBoost_Lung = pickle.load(file)\n```\n\n```{python echo=FALSE}\n# 重要變數\nfeatureplot(RandomForest_Bone.feature_importances_,20, colnames, \"Bone (RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Brain.feature_importances_,20, colnames, \"Brain (Smote+RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Kidney.feature_importances_,20, colnames, \"Kidney (RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Liver.feature_importances_,20, colnames, \"Liver (RandomForest) Feature Importance\")\nfeatureplot(CatBoost_Lung.feature_importances_,20, colnames, \"Lung (CatBoost) Feature Importance\")\n```\n\n- 預測是否轉移至「骨頭」，重要的代謝體特徵為: **C34.4.PC**\n- 預測是否轉移至「大腦」，重要的代謝體特徵為: **C54.6.TAG**, **C58.8.TAG**, **asparagine**, **C56.6.TAG**, **C54.4.TAG**\n- 預測是否轉移至「腎臟」，重要的代謝體特徵為: **homocysteine**, **cytidine**\n- 預測是否轉移至「肝臟」，重要的代謝體特徵為: **GABA**, **putrescine**, **cytidine**\n- 預測是否轉移至「肺臟」，重要的代謝體特徵為: **taurodeoxycholate.taurochenodeoxycholate**, **C36.1.PC**, **C54.6.TAG**, **succinate.methylmalonate**","srcMarkdownNoYaml":"\n### 目標\n預測五個變數(Bone, Brain, Kidney, Liver, Lung)是否有轉移。\n\n### 方法\n- 深度學習: Multi-head Neural Network\n- 機器學習: Decision Tree, Random Forest, LightGBM, CatBoost   \n\n### 預處理 \n#### 劃分資料集\n將資料按照9:1切分訓練集與測試集。針對五個目標變數的轉移狀態，此資料共有28種可能的狀態組合。採用分層切分的方式，確保各個組合在訓練集與測試集的比例盡量保持一致。  \n```{python}\n#| echo: false\n# preprocessing\nimport pandas as pd\nfrom preprocess import preprocess_data, stratify_plot, stratify_stack_plot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n```{python}\n#| echo: false\n# 載入資料\ngenes_path = 'Metastasis/genes.csv'\nmeta_path = 'Metastasis/metabolites.csv'\nresp_path = 'Metastasis/Response.csv'\n\n# 處理資料\nX_train, X_test, y_train, y_test, colnames = preprocess_data(genes_path, meta_path, resp_path, stratify=True)\n# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\ncancers = ['Bone', 'Brain', 'Kidney', 'Liver', 'Lung']\n\n# 切分資料集後 有轉移&無轉移的比例\nstratify_stack_plot(y_train, y_test) \n```\n\n由上圖發現，`Brain`存在些許資料不平衡問題(1.89倍)，其餘變數則不是很明顯，後續分析將嘗試針對`Brain`變數做上採樣(SMOTE)。\n\n\n### Multi-head Neural Network\n#### 目標 \n希望做到一個模型同時預測多個目標變數。  \n\n#### 模型設計\n##### 共享層\n包含一個全連接層，配合ReLU激活函數，其主要作用是提取輸入資料中的通用特徵。這層的輸出維度設定為128維。\n\n##### 五個獨立的輸出層\n在共享層之後，架構分出五個獨立的輸出層，每個輸出層都由一個全連接層構成。每個輸出層都專門負責預測一個特定的目標變數。這種設計允許模型對每個目標進行專門的學習和預測，同時基於共享層的特徵，加強模型對各目標之間可能存在的隱含關聯的理解。\n\n##### 損失函數\n採用二元交叉熵（Binary Cross-Entropy）作為損失函數，對每個獨立輸出層的預測結果計算損失。由於這是一個多目標的預測任務，模型會計算所有輸出層的損失總和，以此來進行梯度下降並更新網路的權重。\n\n##### 優化器\nAdam\n\n##### 學習率\n0.0005  \n\n![Multi-head Neural Network](plot/NN.png){width=\"50%\" align=\"center\"}  \n共有 $4659 \\times 128 + 128 + (128 + 1) \\times 5 = 597125$ 個參數待估計。\n\n#### 模型表現\n```{python}\n#| echo: false\n# DL\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom dl import MultiHeadNN, train, test, evaluate, evaluate_plot\nimport numpy as np\nimport random\n```\n\n```{python}\n#| echo: false\n# 設定隨機種子\nseed = 123\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# 超參數 \nshared = 128 # 共享層維度\nnum_epochs = 200\nbatch_size = 4\ntrytry = 2 # 第幾次測試\n\n# 轉換為 PyTorch 張量\nX_train_tensor = torch.tensor(X_train).float()\nX_test_tensor = torch.tensor(X_test).float()\ny_train_tensor = torch.tensor(y_train).float()\ny_test_tensor = torch.tensor(y_test).float()\n\n# 初始化模型、損失函數和優化器\nmodel = MultiHeadNN(input_dim=X_train_tensor.shape[1], shared_dim=shared, output_dim=1)\ncriterion = nn.BCEWithLogitsLoss() # sigmoid + BCELoss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n```{python}\n#| echo: false\n# 開始訓練\nperformance_history = []\nloss_history = []\nfor epoch in range(num_epochs):\n    # 訓練模型\n    loss = train(model, X_train_tensor, y_train_tensor, criterion, optimizer, batch_size)\n    # 測試模型\n    outputs = test(model, X_test_tensor)\n    # 評估模型\n    epoch_metrics = evaluate(outputs, y_test_tensor.cpu().numpy(), cancers)\n    \n    loss_history.append(loss)\n    performance_history.append(epoch_metrics)\n    formatted_auc = ', '.join([f\"{i}: {epoch_metrics[i]['AUC']:.4f}\" for i in epoch_metrics.keys()])\n    # print(f'【Epoch】{epoch+1}/{num_epochs}, 【Train Loss】{loss:.4f}, 【Test AUC】{formatted_auc}')\n\n\n# 儲存模型\ntorch.save(model, f'model/{trytry}_model.pt')\n# print(f\"已儲存模型至model/{trytry}_model.pt\")\n\n# 匯入模型\n# model = torch.load('model/1_model.pt')\n```\n\n```{python}\n#| echo: false\n# 繪製Loss, AUC\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(num_epochs), loss_history)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(f\"Training Loss\")\nplt.subplot(1, 2, 2)\nevaluate_plot(performance_history, \"AUC\", cancers)\n\nplt.savefig(f'plot/lossAUC_{trytry}.png', dpi=300)\n# print(f\"已儲存圖片至plot/lossAUC_{trytry}.png\")\n```\n```{python}\n#| echo: false\n# 繪製ACC, Precision, Recall, F1\nplt.figure(figsize=(18, 4))\nplt.subplot(1, 4, 1)\nevaluate_plot(performance_history, \"ACC\", cancers)\nplt.subplot(1, 4, 2)\nevaluate_plot(performance_history, \"Precision\", cancers)\nplt.subplot(1, 4, 3)\nevaluate_plot(performance_history, \"Recall\", cancers)\nplt.subplot(1, 4, 4)\nevaluate_plot(performance_history, \"F1\", cancers)\n\nplt.savefig(f'plot/other_{trytry}.png', dpi=300)\n# print(f\"已儲存圖片至plot/other_{trytry}.png\")\n```\n\n\n深度學習模型對隨機初始值特別敏感，尤其當樣本數較少時，這一點更為明顯。在此次實驗的五個目標變數的預測中，AUC值的波動範圍從0.4到0.75不等。但若是初始值設定不佳，模型有時會在初期傾向將所有樣本預測為全正或全負，導致預測結果極不穩定。因此，在樣本量有限的情況下，依賴深度學習可能不是理想的選擇。  \n\n\n### Machine Learning Method\n#### 目標\n為目標變數: Bone, Brain, Kidney, Liver, Lung 分別建立五個模型，並觀察對這五個目標最有影響的變數。  \n\n#### 方法\n由於希望考慮交互作用項，以及後續方便解釋特徵重要性，這裡皆使用Tree-based 模型。\n\n- Decision Tree\n- Random Forest \n- LightGBM\n- CatBoost  \n```{python}\n#| echo: false\n# ML\nfrom ml import check_transfer, choose_model, featureplot, afterSMOTE_models\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nimport pickle\n```\n\n#### Decision Tree\n```{python}\n#| echo: false\n# 決策樹\ndt_results = check_transfer(X_train, X_test, y_train, y_test, \"DecisionTree\")\ndt_results\n```\n\n#### Random Forest\n```{python}\n#| echo: false\n# 隨機森林\nrf_results = check_transfer(X_train, X_test, y_train, y_test, \"RandomForest\")\nrf_results\n```\n\n#### LightGBM\n```{python}\n#| echo: false\n# LightGBM\n# lgbm_results = check_transfer(X_train, X_test, y_train, y_test, \"LightGBM\") # 於server上運算\nlgbm_results = pd.read_csv('result/LightGBM_transfer.csv', index_col=0)\nlgbm_results\n```\n\n#### CatBoost\n```{python}\n#| echo: false\n# CatBoost (計算時間較長 約7分鐘)\n# catb_results = check_transfer(X_train, X_test, y_train, y_test, \"CatBoost\") # 於server上運算\ncatb_results = pd.read_csv('result/CatBoost_transfer.csv', index_col=0)\ncatb_results\n```\n\n#### SMOTE\n僅針對`Brain`變數做上採樣，將少類的樣本補到多數類的80%。\n```{python}\n#| echo: false\n# SMOTE (只做Brain部位)\nsmote = SMOTE(random_state=0, sampling_strategy=0.8)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train[:,1])\n\nprint('Brain')\nprint(f'原資料: 轉移 {np.bincount(y_train[:,1])[0]}, 沒轉移 {np.bincount(y_train[:,1])[1]} ')\nprint(f\"SMOTE後: 轉移 {np.bincount(y_train_smote)[0]}, 沒轉移 {np.bincount(y_train_smote)[1]} \")\n\nmodels = ['DecisionTree', 'RandomForest', 'LightGBM', 'CatBoost']\n# smo_df = afterSMOTE_models(models, 'Brain', X_train_smote, X_test, y_train_smote, y_test) # 於server上運算\n\nsmo_df = pd.read_csv('result/Brain_smote.csv', index_col=0)\nsmo_df\n```\n\n將做過SMOTE上採樣的資料進行訓練，所有模型的表現皆提高了。除了DecisionTree只有微幅上升以外，其餘模型RandomForest, LightGBM, CatBoost皆提高了0.2以上。\n\n#### 各個目標變數對應AUC最高之模型\n```{python}\n#| echo: false\n# 選擇AUC最高的模型\ndf = choose_model(dt_results, rf_results, lgbm_results, catb_results, smo_df)\ndf\n```\n\n以下為針對不同目標變數預測轉移的最佳模型選擇： \n\n- 對於是否轉移到「骨頭」，**隨機森林**模型表現最佳，AUC可達66.56%。\n- 對於是否轉移到「大腦」，結合**SMOTE**與**隨機森林**模型能夠達到最佳效果，AUC可達76.95%。\n- 對於是否轉移到「腎臟」，**隨機森林**模型表現最佳，AUC可達71.15%。\n- 對於是否轉移到「肝臟」，「**隨機森林**模型表現最佳，AUC可達65.38%。\n- 對於是否轉移到「肺臟」，**CatBoost**模型表現最佳，AUC為56.41%。\n\n\n\n### 對是否轉移影響最大的前20個變數\n```{python}\n#| echo: false\nwith open('model/Bone_RandomForest.pkl', 'rb') as file:\n    RandomForest_Bone = pickle.load(file)\nwith open('model/Brain_RandomForest.pkl', 'rb') as file:\n    RandomForest_Brain = pickle.load(file)\nwith open('model/Kidney_RandomForest.pkl', 'rb') as file:\n    RandomForest_Kidney = pickle.load(file)\nwith open('model/Liver_RandomForest.pkl', 'rb') as file:\n    RandomForest_Liver = pickle.load(file)\nwith open('model/Lung_CatBoost.pkl', 'rb') as file:\n    CatBoost_Lung = pickle.load(file)\n```\n\n```{python echo=FALSE}\n# 重要變數\nfeatureplot(RandomForest_Bone.feature_importances_,20, colnames, \"Bone (RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Brain.feature_importances_,20, colnames, \"Brain (Smote+RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Kidney.feature_importances_,20, colnames, \"Kidney (RandomForest) Feature Importance\")\nfeatureplot(RandomForest_Liver.feature_importances_,20, colnames, \"Liver (RandomForest) Feature Importance\")\nfeatureplot(CatBoost_Lung.feature_importances_,20, colnames, \"Lung (CatBoost) Feature Importance\")\n```\n\n- 預測是否轉移至「骨頭」，重要的代謝體特徵為: **C34.4.PC**\n- 預測是否轉移至「大腦」，重要的代謝體特徵為: **C54.6.TAG**, **C58.8.TAG**, **asparagine**, **C56.6.TAG**, **C54.4.TAG**\n- 預測是否轉移至「腎臟」，重要的代謝體特徵為: **homocysteine**, **cytidine**\n- 預測是否轉移至「肝臟」，重要的代謝體特徵為: **GABA**, **putrescine**, **cytidine**\n- 預測是否轉移至「肺臟」，重要的代謝體特徵為: **taurodeoxycholate.taurochenodeoxycholate**, **C36.1.PC**, **C54.6.TAG**, **succinate.methylmalonate**"},"formats":{"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":false,"include-in-header":[{"text":"\\usepackage{setspace,relsize}\n\\usepackage{xeCJK}\n\\setCJKmainfont{STSong}\n"}],"output-file":"group3_MLDL_run.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"subtitle":"Group 3","date":"today","author":"尹子維、李承祐、黃亮臻、張立勳","mainfont":"STSong","bibliography":["../references.bib"],"title":"Analysis using ML, DL","pdf_document":{"latex_engine":"xelatex","geometry":["left=30mm","right=30mm","top=30mm","bottom=30mm"]}},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html"]}